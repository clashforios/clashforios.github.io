<!doctype html>
<html xml:lang="zh-CN" lang="zh-CN">

<head>
        <link rel="canonical" href="https://clashforios.github.io/news/article-109875.htm" />
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>从源码看ONNXRuntime的执行流程</title>
        <meta name="description" content="目录  前言 准备工作 构造 InferenceSession 对象 &amp; 初始化 让模型 Run 总结   前言 在上一篇博客中：【推理引擎】ONNXRuntime 的架构设计，主要从文档上对" />
        <link rel="icon" href="/assets/website/img/clashforios/favicon.ico" type="image/x-icon"/>

    <meta name="author" content="Clash for IOS免费机场订阅节点官网">
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://clashforios.github.io/news/article-109875.htm" />
    <meta property="og:site_name" content="Clash for IOS免费机场订阅节点官网" />
    <meta property="og:title" content="从源码看ONNXRuntime的执行流程" />
    <meta property="og:image" content="https://clashforios.github.io/uploads/20240922-1/08b23ed63c524bdfee2572515b8b88ad.webp" />
        <meta property="og:release_date" content="2025-05-02T09:23:55" />
    <meta property="og:updated_time" content="2025-05-02T09:23:55" />
        <meta property="og:description" content="目录  前言 准备工作 构造 InferenceSession 对象 &amp; 初始化 让模型 Run 总结   前言 在上一篇博客中：【推理引擎】ONNXRuntime 的架构设计，主要从文档上对" />
        
    <meta name="applicable-device" content="pc,mobile" />
    <meta name="renderer" content="webkit" />
    <meta name="force-rendering" content="webkit" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta name="robots" content="max-image-preview:large" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="从源码看ONNXRuntime的执行流程">
    <meta name="format-detection" content="telephone=no">

    <link rel="dns-prefetch" href="https:/www.googletagmanager.com">
    <link rel="dns-prefetch" href="https://www.googleadservices.com">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link rel="dns-prefetch" href="https://pagead2.googlesyndication.com">
    <link rel="dns-prefetch" href="https://cm.g.doubleclick.net">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/bootstrap.min.css">
    <!-- Slick Silder CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/slick.css">
    <!-- Fontawesome CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/fontawesome.css">
    <!-- Flaticon CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/style.css">
    <!-- Theme Element CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/responsivemenu.css">
    <!-- Theme Element CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/elements.css">
    <!-- Color CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/color.css">
    <!-- Responsive CSS -->
    <link rel="stylesheet" href="/assets/website/css/clashforios/responsive.css">
    <link rel="stylesheet" href="/assets/website/css/G.css" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-547GV0YTCL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-547GV0YTCL');
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
    
</head>

<body data-page="detail">
    <!-- Main Wrapper Start -->
    <div class="main-wrapper">
                <header id="header-1" class="header-2">
            <div class="sticky-header">
                <div class="container">
                                        <a href="/" class="logo float-left">
                        <span>
                            Clash for IOS                        </span>
                    </a>
                                        <!-- /.logo End -->
                    <div class="navigation-wrap float-right">
                        <div class="menu-holder">
                            <div class="mobile-menu"></div>
                            <!-- /.mobile-menu End -->
                            <div class="main-menu navigation">
                                <nav>
                                    <ul>
                                                                                <li><a href="/">首页</a></li>
                                                                                <li><a href="/free-nodes/">免费节点</a></li>
                                                                                <li><a href="/paid-subscribe/">推荐机场</a></li>
                                                                                <li><a href="/client.htm">客户端</a></li>
                                                                                <li><a href="/news/">新闻资讯</a></li>
                                                                            </ul>
                                </nav>
                            </div>
                            <!-- /.navigation End -->
                        </div>
                        <!-- /.menu-holder End -->
                    </div>
                </div>
            </div>
        </header>
        <!-- /.header-2 End -->
        <!-- Sub Banner Start -->
        <div class="sub-banner">
            <div class="container">
                <div class="text text-right">
                    <h1 class="title">从源码看ONNXRuntime的执行流程</h1>
                </div>
                <nav class="breadcrumb-outer" aria-label="breadcrumb">
                    <ol class="breadcrumb d-inline-flex">
                        <li class="breadcrumb-item"><a href="/">首页</a></li>
                        <li class="breadcrumb-item"><a href="/news/">新闻资讯</a></li>
                        <li class="breadcrumb-item active" aria-current="page">正文</li>
                    </ol>
                </nav>
            </div>
        </div>
        <!-- Sub Banner End -->
        <!-- Main Content Start -->
        <div class="main-content">
            <!-- About us Section Start -->
            <section class="about-us mb-30">
                <div class="container">
                    <div class="row">
                        <div class="col-md-9">
                                            <input type="hidden" id="share-website-info" data-name="Clash Node官网订阅站" data-url="https://clashnode.github.io">
                <div class="xcblog-blog-detail xcblog-blog-detail-defined">
                      				  				  				<div class="toc"> <div class="toc-container-header">目录</div> <ul> <li>前言</li> <li>准备工作</li> <li>构造 InferenceSession 对象 &amp; 初始化</li> <li>让模型 Run</li> <li>总结</li> </ul> </div> <h2 id="前言">前言</h2> <p>在上一篇博客中：【推理引擎】ONNXRuntime 的架构设计，主要从文档上对ONNXRuntime的执行流程进行了梳理，但是想要深入理解，还需从源码角度进行分析。</p> <p>本文以目标检测模型NanoDet作为分析的基础，部分代码主要参考：超轻量级NanoDet MNN/TNN/NCNN/ONNXRuntime C++工程记录 - DefTruth的文章 - 知乎，在此表示感谢！</p> <h2 id="准备工作">准备工作</h2> <p><code>OrtHandlerBase</code> 是用来操控 ONNXRuntime 的基类，各种网络模型都可以通过继承该类进而拥有 ONNXRuntime 的使用权限，比如<code>NanoDet</code>；同时，<code>NanoDet</code>还可以扩展独属于自己的方法和成员变量，以方便推理前后的预处理和后处理工作。<br /><img decoding="async" src="http://img.555519.xyz/uploads3/20220510/c15b04de0be7c1fd2d458237183edfb6.jpg" alt="从源码看ONNXRuntime的执行流程"></p> <p>构造<code>NanoDet</code>对象时，会自动调用<code>OrtHandlerBase</code>的构造方法，在构造方法内部会首先初始化一些必要的成员变量（<code>Ort::Env</code>、<code>Ort::SessionOptions</code>），这两个变量主要用于初始化<code>Ort::Session</code>:</p> <pre><code class="language-cpp">ort_env = Ort::Env(ORT_LOGGING_LEVEL_ERROR, log_id);  Ort::SessionOptions session_options; session_options.SetIntraOpNumThreads(num_threads); session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL); session_options.SetLogSeverityLevel(4);  ort_session = new Ort::Session(ort_env, onnx_model_path, session_options);</code></pre> <h2 id="构造-inferencesession-对象--初始化">构造 InferenceSession 对象 &amp; 初始化</h2> <p>在构造<code>Ort::Session</code> 对象的过程中，会调用ONNXRuntime -&gt; onnxruntime_cxx_inline.h 中的API：</p> <pre><code class="language-cpp">// include/onnxruntime/core/session/onnxruntime_cxx_inline.h inline Session::Session(Env&amp; env, const ORTCHAR_T* model_path, const SessionOptions&amp; options) {   ThrowOnError(GetApi().CreateSession(env, model_path, options, &amp;p_)); }</code></pre> <p>GetApi() 是在 onnxruntime_cxx_api.h 中定义的：</p> <pre><code class="language-cpp">// include/onnxruntime/core/session/onnxruntime_cxx_api.h  // This returns a reference to the OrtApi interface in use inline const OrtApi&amp; GetApi() { return *Global&lt;void&gt;::api_; }  // 其中 Global 的定义如下： template &lt;typename T&gt; struct Global {   static const OrtApi* api_; };</code></pre> <p>这里面主要定义了静态常量指针<code>OrtApi*</code>，<code>OrtApi</code>是在 onnxruntime_c_api.h 中定义的：</p> <pre><code class="language-cpp">// include/onnxruntime/core/session/onnxruntime_c_api.h  // All C API functions are defined inside this structure as pointers to functions. // Call OrtApiBase::GetApi to get a pointer to it struct OrtApi; typedef struct OrtApi OrtApi;  struct OrtApi{   ...   // 以 CreateSession 为例：   ORT_API2_STATUS(CreateSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,                   _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);    // 展开ORT_API2_STATUS宏：   // _Check_return_ _Ret_maybenull_ OrtStatusPtr(ORT_API_CALL* CreateSession)(const OrtEnv* env,    //                                                                          const char* model_path,    //                                                                          const OrtSessionOptions* options,    //                                                                          OrtSession** out) NO_EXCEPTION ORT_MUST_USE_RESULT;      ... }</code></pre> <p>相应地，在 onnxruntime_c_api.cc 文件中定义了 CreateSesssion 的实现：</p> <pre><code class="language-cpp">// onnxruntime/core/session/onnxruntime_c_api.cc  ORT_API_STATUS_IMPL(OrtApis::CreateSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,                     _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out) {   API_IMPL_BEGIN   std::unique_ptr&lt;onnxruntime::InferenceSession&gt; sess;   OrtStatus* status = nullptr;   *out = nullptr;    ORT_TRY {     ORT_API_RETURN_IF_ERROR(CreateSessionAndLoadModel(options, env, model_path, nullptr, 0, sess));     ORT_API_RETURN_IF_ERROR(InitializeSession(options, sess));      *out = reinterpret_cast&lt;OrtSession*&gt;(sess.release());   }   ORT_CATCH(const std::exception&amp; e) {     ORT_HANDLE_EXCEPTION([&amp;]() {       status = OrtApis::CreateStatus(ORT_FAIL, e.what());     });   }    return status;   API_IMPL_END }</code></pre> <p>到此，我们已经定位到CreateSession的具体实现内容，可以发现它主要由两个部分组成：<code>CreateSessionAndLoadModel</code> 和<code>InitializeSession</code>，接下来分析这两个函数。</p> <p>从<code>CreateSessionAndLoadModel</code> 的名字就可以看出，这个函数主要负责创建 Session，以及加载模型：</p> <pre><code class="language-cpp">// onnxruntime/core/session/onnxruntime_c_api.cc  // provider either model_path, or modal_data + model_data_length. // 也就是说，共有两种方式用来读取模型：一种是根据ONNX模型路径；另一种时从模型数据缓冲（Model data buffer）中读取，并且需要指定模型大小（Model data buffer size） static ORT_STATUS_PTR CreateSessionAndLoadModel(_In_ const OrtSessionOptions* options,                                                 _In_ const OrtEnv* env,                                                 _In_opt_z_ const ORTCHAR_T* model_path,                                                 _In_opt_ const void* model_data,                                                 size_t model_data_length,                                                 std::unique_ptr&lt;onnxruntime::InferenceSession&gt;&amp; sess) {   // quick check here to decide load path. InferenceSession will provide error message for invalid values.   const Env&amp; os_env = Env::Default();  // OS environment (注意：OS environment != ORT environment)   bool load_config_from_model =       os_env.GetEnvironmentVar(inference_session_utils::kOrtLoadConfigFromModelEnvVar) == "1";      // 创建 InferenceSession   if (load_config_from_model) {     if (model_path != nullptr) {       sess = std::make_unique&lt;onnxruntime::InferenceSession&gt;(           options == nullptr ? onnxruntime::SessionOptions() : options-&gt;value,           env-&gt;GetEnvironment(),           model_path);     } else {       sess = std::make_unique&lt;onnxruntime::InferenceSession&gt;(           options == nullptr ? onnxruntime::SessionOptions() : options-&gt;value,           env-&gt;GetEnvironment(),           model_data, static_cast&lt;int&gt;(model_data_length));     }   } else {     sess = std::make_unique&lt;onnxruntime::InferenceSession&gt;(         options == nullptr ? onnxruntime::SessionOptions() : options-&gt;value,         env-&gt;GetEnvironment());   }    // Add custom domains   if (options &amp;&amp; !options-&gt;custom_op_domains_.empty()) {     ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;AddCustomOpDomains(options-&gt;custom_op_domains_));   }    // Finish load   if (load_config_from_model) {     ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;Load());   } else {     if (model_path != nullptr) {       ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;Load(model_path));     } else {       ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;Load(model_data, static_cast&lt;int&gt;(model_data_length)));     }   }    return nullptr; }</code></pre> <p>接下来深入到<code>sess-&gt;load()</code> 中，这里面经历了多层重载函数，最终目标是为InferenceSession的成员变量model_（ClassType: std::shared_ptronnxruntime::Model）赋值：</p> <pre><code class="language-cpp">// onnxruntime/core/session/onnxruntime_c_api.cc  common::Status InferenceSession::Load(const std::string&amp; model_uri) {   std::string model_type = session_options_.config_options.GetConfigOrDefault(kOrtSessionOptionsConfigLoadModelFormat, "");   bool has_explicit_type = !model_type.empty();      // 判断是否为 ORT 类型的 Model   if ((has_explicit_type &amp;&amp; model_type == "ORT") ||       (!has_explicit_type &amp;&amp; fbs::utils::IsOrtFormatModel(model_uri))) {     return LoadOrtModel(model_uri);   }    if (is_model_proto_parsed_) {     return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL,                            "ModelProto corresponding to the model to be loaded has already been parsed. "                            "Invoke Load().");   }    return Load&lt;char&gt;(model_uri); }  template &lt;typename T&gt; common::Status InferenceSession::Load(const std::basic_string&lt;T&gt;&amp; model_uri) {   model_location_ = ToWideString(model_uri);   // 这里定义了一个 lambda 函数   auto loader = [this](std::shared_ptr&lt;onnxruntime::Model&gt;&amp; model) {     LoadInterOp(model_location_, interop_domains_, [&amp;](const char* msg) { LOGS(*session_logger_, WARNING) &lt;&lt; msg; });     for (const auto&amp; domain : interop_domains_) {       ORT_RETURN_IF_ERROR(AddCustomOpDomains({domain.get()}));     }     return onnxruntime::Model::Load(model_location_, model, HasLocalSchema() ? &amp;custom_schema_registries_ : nullptr,                                     *session_logger_);   };    common::Status st = Load(loader, "model_loading_uri");    return Status::OK(); }  common::Status InferenceSession::Load(std::function&lt;common::Status(std::shared_ptr&lt;Model&gt;&amp;)&gt; loader,                                       const std::string&amp; event_name) {   std::lock_guard&lt;onnxruntime::OrtMutex&gt; l(session_mutex_);   // 关键代码   std::shared_ptr&lt;onnxruntime::Model&gt; p_tmp_model;   status = loader(p_tmp_model);   model_ = p_tmp_model;   status = DoPostLoadProcessing(*model_);      is_model_loaded_ = true;   return status; }</code></pre> <p>需要注意的是，onnxruntime::Model 不同于 onnxruntime::Graph，Graph 只是 Model 的一个成员变量，Model 中还包含其它基础信息，比如 version、domain、author 和 license 等内容。</p> <p>在创建完 InferenceSession 后，需要进行初始化操作（<code>InitializeSession</code>）：</p> <pre><code class="language-cpp">// onnxruntime/core/session/onnxruntime_c_api.cc  static ORT_STATUS_PTR InitializeSession(_In_ const OrtSessionOptions* options,                                         _In_ std::unique_ptr&lt;::onnxruntime::InferenceSession&gt;&amp; sess,                                         _Inout_opt_ OrtPrepackedWeightsContainer* prepacked_weights_container = nullptr) {   // 创建 Providers   std::vector&lt;std::unique_ptr&lt;IExecutionProvider&gt;&gt; provider_list;   if (options) {     for (auto&amp; factory : options-&gt;provider_factories) {       auto provider = factory-&gt;CreateProvider();       provider_list.push_back(std::move(provider));     }   }    // 注册 Providers 到 InferenceSession 中   for (auto&amp; provider : provider_list) {     if (provider) {       ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;RegisterExecutionProvider(std::move(provider)));     }   }    if (prepacked_weights_container != nullptr) {     ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;AddPrePackedWeightsContainer(         reinterpret_cast&lt;PrepackedWeightsContainer*&gt;(prepacked_weights_container)));   }      // 初始化 InferenceSession   ORT_API_RETURN_IF_STATUS_NOT_OK(sess-&gt;Initialize());    return nullptr; }</code></pre> <p>接下来，深入到 InferenceSession 的<code>Initialize()</code> 函数中，这个函数水很深，需要分为几个小的模块来分析。</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    bool have_cpu_ep = false;      // 这里使用 {} 可以提前释放 session_mutex_，不必等到退出Initialize函数才释放，可提升效率   {        std::lock_guard&lt;onnxruntime::OrtMutex&gt; initial_guard(session_mutex_);     // 判断模型是否已被加载     if (!is_model_loaded_) {           LOGS(*session_logger_, ERROR) &lt;&lt; "Model was not loaded";       return common::Status(common::ONNXRUNTIME, common::FAIL, "Model was not loaded.");     }      if (is_inited_) {  // 判断是否已经初始化，如果已经初始化就可以直接退出Initialize函数了       LOGS(*session_logger_, INFO) &lt;&lt; "Session has already been initialized.";       return common::Status::OK();     }          // 判断是否已经设置 CPU EP 来兜底，如果忘记设置，后面会自动添加     have_cpu_ep = execution_providers_.Get(onnxruntime::kCpuExecutionProvider) != nullptr;   }    if (!have_cpu_ep) {     LOGS(*session_logger_, INFO) &lt;&lt; "Adding default CPU execution provider.";     CPUExecutionProviderInfo epi{session_options_.enable_cpu_mem_arena};     auto p_cpu_exec_provider = std::make_unique&lt;CPUExecutionProvider&gt;(epi);     ORT_RETURN_IF_ERROR_SESSIONID_(RegisterExecutionProvider(std::move(p_cpu_exec_provider)));   }   ... }</code></pre> <p>以上代码确保了 EPs（复数，多个EP，hhh） 已被正常设置（主要是CPU已经被用作兜底），接下来从 Ort 环境中读取共享的分配器（shared allocators），并更新 EPs：</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    std::string use_env_allocators = session_options_.config_options.GetConfigOrDefault(kOrtSessionOptionsConfigUseEnvAllocators,                                                                                       "0");   if (use_env_allocators == "1") {     LOGS(*session_logger_, INFO) &lt;&lt; "This session will use the allocator registered with the environment.";     UpdateProvidersWithSharedAllocators();    // 更新 EPs   }    ...</code></pre> <p>接下来需要设定 SessionState，需要注意：SessionState 只能被 InferenceSession 修改，</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    session_state_ = std::make_unique&lt;SessionState&gt;(       model_-&gt;MainGraph(),       execution_providers_,       session_options_.enable_mem_pattern &amp;&amp; session_options_.execution_mode == ExecutionMode::ORT_SEQUENTIAL,       GetIntraOpThreadPoolToUse(),       GetInterOpThreadPoolToUse(),       data_transfer_mgr_,       *session_logger_,       session_profiler_,       session_options_.use_deterministic_compute,       session_options_.enable_mem_reuse,       prepacked_weights_container_);      ... }</code></pre> <p>接下来从EPs实例中收集内核注册表（kernel registries），内核注册表分为两类：</p> <ol> <li>Custom execution provider type specific kernel registries.     》》 比如CUDA EP</li> <li>Common execution provider type specific kernel registries.     》》 比如CPU EP</li> </ol> <p>这两类注册表的优先级并不相同，前者要高于后者。</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    ORT_RETURN_IF_ERROR_SESSIONID_(kernel_registry_manager_.RegisterKernels(execution_providers_));      ... }</code></pre> <p>在 KernelRegistryManager 中注册完注册表之后，开始执行非常重要的图优化，以及分割子图：</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    // add predefined transformers   // 添加预先定义的变换   ORT_RETURN_IF_ERROR_SESSIONID_(AddPredefinedTransformers(graph_transformation_mgr_,                                                             session_options_.graph_optimization_level,                                                             saving_runtime_optimizations));    // apply any transformations to the main graph and any subgraphs   // 在主图和子图上执行所有的优化Pass   ORT_RETURN_IF_ERROR_SESSIONID_(TransformGraph(graph, graph_transformation_mgr_,                                                 execution_providers_, kernel_registry_manager_,                                                 insert_cast_transformer_,                                                 *session_state_,                                                 saving_ort_format));    // now that all the transforms are done, call Resolve on the main graph. this will recurse into the subgraphs.   // 所有的图变换都已经执行完毕，然后开始递归分割子图   ORT_RETURN_IF_ERROR_SESSIONID_(graph.Resolve());    // Update temporary copies of metadata, input- and output definitions to the same state as the resolved graph   ORT_RETURN_IF_ERROR_SESSIONID_(SaveModelMetadata(*model_));    ... }</code></pre> <p>分割子图之后，还有一些结尾工作：</p> <pre><code class="language-cpp">// onnxruntime/core/session/inference_session.cc  common::Status InferenceSession::Initialize() {   ...    ORT_RETURN_IF_ERROR_SESSIONID_(       session_state_-&gt;FinalizeSessionState(model_location_, kernel_registry_manager_,                                             session_options_,                                             serialized_session_state,                                             // need to keep the initializers if saving the optimized model                                             !saving_model,                                             saving_ort_format));      // Resolve memory pattern flags of the main graph and subgraph session states   ResolveMemoryPatternFlags(*session_state_);    // 在 session 创建完成之后，分别调用各个EP的OnSessionInitializationEnd方法，这一步主要为EP提供一个机会，进行选择性地同步或者清理临时资源   // 从而减少内存占用，确保第一次运行时足够快   if (status.IsOK()) {     for (auto&amp; xp : execution_providers_) {       auto end_status = xp-&gt;OnSessionInitializationEnd();       if (status.IsOK()) {         status = end_status;       }     }   }      return status; }</code></pre> <h2 id="让模型-run">让模型 Run</h2> <p>通过上一个阶段，已经成功构造出 NanoDet 对象，接下来需要输入图像，并由 NanoDet 来执行：</p> <pre><code class="language-cpp">//  std::vector&lt;types::BoxF&gt; detected_boxes; cv::Mat img_bgr = cv::imread(test_img_path); nanodet-&gt;detect(img_bgr, detected_boxes);</code></pre> <p>detect 函数内部：</p> <pre><code class="language-cpp">void NanoDet::detect(const cv::Mat &amp;mat, std::vector&lt;types::BoxF&gt; &amp;detected_boxes,                      float score_threshold, float iou_threshold,                      unsigned int topk, unsigned int nms_type) {     if (mat.empty()) return;     auto img_height = static_cast&lt;float&gt;(mat.rows);     auto img_width = static_cast&lt;float&gt;(mat.cols);     const int target_height = (int) input_node_dims.at(2);     const int target_width = (int) input_node_dims.at(3);      // 0. resize &amp; unscale     cv::Mat mat_rs;     NanoScaleParams scale_params;     this-&gt;resize_unscale(mat, mat_rs, target_height, target_width, scale_params);      // 1. make input tensor     Ort::Value input_tensor = this-&gt;transform(mat_rs);          // 2. inference scores &amp; boxes.     auto output_tensors = ort_session-&gt;Run(         Ort::RunOptions{nullptr}, input_node_names.data(),         &amp;input_tensor, 1, output_node_names.data(), num_outputs     );     // 3. rescale &amp; exclude.     std::vector&lt;types::BoxF&gt; bbox_collection;     this-&gt;generate_bboxes(scale_params, bbox_collection, output_tensors, score_threshold, img_height, img_width);     // 4. hard|blend|offset nms with topk.     this-&gt;nms(bbox_collection, detected_boxes, iou_threshold, topk, nms_type); }</code></pre> <p>其中，第 0 和 1 步是模型输入的预处理，这里不再深入介绍，想要了解可参考源码。接下来重点对第 2 步的<code>ort_seesion-&gt;Run()</code> 进行深入剖析。</p> <pre><code class="language-cpp">// include/onnxruntime/core/session/onnxruntime_cxx_inline.h  inline std::vector&lt;Value&gt; Session::Run(const RunOptions&amp; run_options, const char* const* input_names, const Value* input_values, size_t input_count,                                        const char* const* output_names, size_t output_names_count) {   std::vector&lt;Ort::Value&gt; output_values;   for (size_t i = 0; i &lt; output_names_count; i++)     output_values.emplace_back(nullptr);   Run(run_options, input_names, input_values, input_count, output_names, output_values.data(), output_names_count);   return output_values; }  inline void Session::Run(const RunOptions&amp; run_options, const char* const* input_names, const Value* input_values, size_t input_count,                          const char* const* output_names, Value* output_values, size_t output_count) {   static_assert(sizeof(Value) == sizeof(OrtValue*), "Value is really just an array of OrtValue* in memory, so we can reinterpret_cast safely");   auto ort_input_values = reinterpret_cast&lt;const OrtValue**&gt;(const_cast&lt;Value*&gt;(input_values));   auto ort_output_values = reinterpret_cast&lt;OrtValue**&gt;(output_values);   ThrowOnError(GetApi().Run(p_, run_options, input_names, ort_input_values, input_count, output_names, output_count, ort_output_values)); }</code></pre> <p>又到了熟悉的环节，GetApi()可参考上一章节的内容，直接到 onnxruntime_c_api.cc 中查看 Run 函数对应的实现：</p> <pre><code class="language-cpp">// onnxruntime/core/session/onnxruntime_c_api.cc  ORT_API_STATUS_IMPL(OrtApis::Run, _Inout_ OrtSession* sess, _In_opt_ const OrtRunOptions* run_options,                     _In_reads_(input_len) const char* const* input_names,                     _In_reads_(input_len) const OrtValue* const* input, size_t input_len,                     _In_reads_(output_names_len) const char* const* output_names1, size_t output_names_len,                     _Inout_updates_all_(output_names_len) OrtValue** output) {   API_IMPL_BEGIN   // 获取 inferencesession   auto session = reinterpret_cast&lt;::onnxruntime::InferenceSession*&gt;(sess);   const int queue_id = 0;      // 模型输入：feed_names &amp; feeds   std::vector&lt;std::string&gt; feed_names(input_len);   std::vector&lt;OrtValue&gt; feeds(input_len);    for (size_t i = 0; i != input_len; ++i) {     if (input_names[i] == nullptr || input_names[i][0] == '\0') {       return OrtApis::CreateStatus(ORT_INVALID_ARGUMENT, "input name cannot be empty");     }      feed_names[i] = input_names[i];     auto&amp; ort_value = feeds[i] = *reinterpret_cast&lt;const ::OrtValue*&gt;(input[i]);      if (ort_value.Fence()) ort_value.Fence()-&gt;BeforeUsingAsInput(onnxruntime::kCpuExecutionProvider, queue_id);   }    // 模型输出：output_names &amp; fetches   std::vector&lt;std::string&gt; output_names(output_names_len);   for (size_t i = 0; i != output_names_len; ++i) {     if (output_names1[i] == nullptr || output_names1[i][0] == '\0') {       return OrtApis::CreateStatus(ORT_INVALID_ARGUMENT, "output name cannot be empty");     }     output_names[i] = output_names1[i];   }    std::vector&lt;OrtValue&gt; fetches(output_names_len);   for (size_t i = 0; i != output_names_len; ++i) {     if (output[i] != nullptr) {       ::OrtValue&amp; value = *(output[i]);       if (value.Fence())         value.Fence()-&gt;BeforeUsingAsOutput(onnxruntime::kCpuExecutionProvider, queue_id);       fetches[i] = value;     }   }      // 调用 InferenceSession 的 Run 函数，执行推理   Status status;   if (run_options == nullptr) {     OrtRunOptions op;     status = session-&gt;Run(op, feed_names, feeds, output_names, &amp;fetches, nullptr);   } else {     status = session-&gt;Run(*run_options, feed_names, feeds, output_names, &amp;fetches, nullptr);   }      // Run 结束后，将 fetches 中的内容取出放到 output 中   if (!status.IsOK())     return ToOrtStatus(status);   for (size_t i = 0; i != output_names_len; ++i) {     ::OrtValue&amp; value = fetches[i];     if (value.Fence())       value.Fence()-&gt;BeforeUsingAsInput(onnxruntime::kCpuExecutionProvider, queue_id);     if (output[i] == nullptr) {       output[i] = new OrtValue(value);     }   }   return nullptr;   API_IMPL_END }</code></pre> <p>进入到<code>InferenceSession::Run</code> 的内部：</p> <pre><code class="language-cpp">Status InferenceSession::Run(const RunOptions&amp; run_options,                              const std::vector&lt;std::string&gt;&amp; feed_names, const std::vector&lt;OrtValue&gt;&amp; feeds,                              const std::vector&lt;std::string&gt;&amp; output_names, std::vector&lt;OrtValue&gt;* p_fetches,                              const std::vector&lt;OrtDevice&gt;* p_fetches_device_info) {    std::vector&lt;IExecutionProvider*&gt; exec_providers_to_stop;   exec_providers_to_stop.reserve(execution_providers_.NumProviders());    std::vector&lt;AllocatorPtr&gt; arenas_to_shrink;    // 验证输入输出，并由 FeedsFetchesManager 进行管理   ORT_RETURN_IF_ERROR_SESSIONID_(ValidateInputs(feed_names, feeds));   ORT_RETURN_IF_ERROR_SESSIONID_(ValidateOutputs(output_names, p_fetches));   FeedsFetchesInfo info(feed_names, output_names, session_state_-&gt;GetOrtValueNameIdxMap());   FeedsFetchesManager feeds_fetches_manager{std::move(info)};      // current_num_runs_ 的类型是：std::atomic&lt;int&gt;，表示并行运行 EP 的数量   ++current_num_runs_;       // info all execution providers InferenceSession:Run started   for (auto&amp; xp : execution_providers_) {     // call OnRunStart and add to exec_providers_to_stop if successful     auto start_func = [&amp;xp, &amp;exec_providers_to_stop]() {       auto status = xp-&gt;OnRunStart();       if (status.IsOK())         exec_providers_to_stop.push_back(xp.get());        return status;     };      ORT_CHECK_AND_SET_RETVAL(start_func());   }      if (run_options.only_execute_path_to_fetches) {     session_state_-&gt;UpdateToBeExecutedNodes(feeds_fetches_manager.GetFeedsFetchesInfo().fetches_mlvalue_idxs);   }    session_state_-&gt;IncrementGraphExecutionCounter();      // execute the graph   ORT_CHECK_AND_SET_RETVAL(utils::ExecuteGraph(*session_state_, feeds_fetches_manager, feeds, *p_fetches,                                                 session_options_.execution_mode, run_options.terminate, run_logger,                                                 run_options.only_execute_path_to_fetches));    // info all execution providers InferenceSession:Run ended   for (auto* xp : exec_providers_to_stop) {     auto status = xp-&gt;OnRunEnd(/*sync_stream*/ true);     ORT_CHECK_AND_SET_RETVAL(status);   }      --current_num_runs_; }</code></pre> <p>至此，模型已经完成推理，接下来只需处理输出内容即可，对应 nanodet-&gt;detect() 函数的 3、4 部分。</p> <h2 id="总结">总结</h2> <p>本文主要介绍了InferenceSession的构造和初始化，以及模型的推理过程，可以发现其中还是蛮复杂的。由于对ONNXRuntime的源码仍然了解有限，有许多重要的部分被略过，打算接下来分别针对突破。</p> 			                </div>
                <div class="clearfix"></div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/news/article-109022.htm">户外凉鞋品牌 保暖鞋十大名牌排行榜图片(户外凉鞋品牌 保暖鞋十大名牌排行榜及价格)</a></p>
                                        <p>下一个：<a href="/news/article-109876.htm">动物医院取什么名字形容敬业（动物医院有什么）</a></p>
                                    </div>
                                        </div>
                        <div class="col-md-3">
                            <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/free-nodes/2025-4-17-free-node-subscribe-links.htm" title="4月17日→22.3M/S|2025年最新免费节点Clash for IOS订阅链接地址">4月17日→22.3M/S|2025年最新免费节点Clash for IOS订阅链接地址</a></li>
                        <li class="py-2"><a href="/news/article-88344.htm" title="C++引用及其底层原理">C++引用及其底层原理</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-4-7-free-node-subscribe.htm" title="4月7日→20.1M/S|2025年最新免费节点Clash for IOS订阅链接地址">4月7日→20.1M/S|2025年最新免费节点Clash for IOS订阅链接地址</a></li>
                        <li class="py-2"><a href="/news/article-89023.htm" title="宠物粮食批发厂家价格表图片高清（宠物粮市场前景怎么样）">宠物粮食批发厂家价格表图片高清（宠物粮市场前景怎么样）</a></li>
                        <li class="py-2"><a href="/news/article-108194.htm" title="动物疫苗不冷藏可以放多久呢知乎（动物疫苗需要冷藏吗）">动物疫苗不冷藏可以放多久呢知乎（动物疫苗需要冷藏吗）</a></li>
                        <li class="py-2"><a href="/news/article-109022.htm" title="户外凉鞋品牌 保暖鞋十大名牌排行榜图片(户外凉鞋品牌 保暖鞋十大名牌排行榜及价格)">户外凉鞋品牌 保暖鞋十大名牌排行榜图片(户外凉鞋品牌 保暖鞋十大名牌排行榜及价格)</a></li>
                        <li class="py-2"><a href="/news/article-91493.htm" title="「IU」「分享」190613《夜信》自公开至今 Gaon流媒突破两亿五千万">「IU」「分享」190613《夜信》自公开至今 Gaon流媒突破两亿五千万</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-4-19-free-high-speed-nodes.htm" title="4月19日→22.5M/S|2025年最新免费节点Clash for IOS订阅链接地址">4月19日→22.5M/S|2025年最新免费节点Clash for IOS订阅链接地址</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-4-16-free-node-subscribe.htm" title="4月16日→22.4M/S|2025年最新免费节点Clash for IOS订阅链接地址">4月16日→22.4M/S|2025年最新免费节点Clash for IOS订阅链接地址</a></li>
                        <li class="py-2"><a href="/news/article-98664.htm" title="SDS-redis动态字符串">SDS-redis动态字符串</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">12</span> <a href="/date/2025-05/" title="2025-05 归档">2025-05</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">89</span> <a href="/date/2025-04/" title="2025-04 归档">2025-04</a></h4>
            </li>
                    </ul>
    </div>
</div>

                        </div>
                    </div>
                </div>
            </section>
            <!-- About us Section End -->
        </div>
        <!-- Main Content End -->
                <!-- Copyright Start -->
        <div class="copyright">
                                <p>
                                                <a href="/">首页</a> |
                                                <a href="/free-nodes/">免费节点</a> |
                                                <a href="/paid-subscribe/">推荐机场</a> |
                                                <a href="/client.htm">客户端</a> |
                                                <a href="/news/">新闻资讯</a> |
                                                <a href="/about-us.htm">关于我们</a> |
                        <a href="/disclaimer.htm">免责申明</a> |
                        <a href="/privacy.htm">隐私申明</a> |
                        <a href="/sitemap.xml">网站地图</a>
                    </p>
            <p>
                <a href="/">Clash for IOS免费机场订阅节点官网</a> 版权所有 Powered by WordPress
            </p>
        </div>
        <!-- Copyright End -->
        <!-- Optional JavaScript -->
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="/assets/website/js/frontend/clashforios/jquery.js"></script>
        <script src="/assets/website/js/frontend/clashforios/popper.min.js"></script>
        <script src="/assets/website/js/frontend/clashforios/bootstrap.min.js"></script>
        <script src="/assets/website/js/frontend/clashforios/responsivemenu.js"></script>
        <script src="/assets/website/js/frontend/clashforios/slick.min.js"></script>
        <script src="/assets/website/js/frontend/clashforios/masonry.min.js"></script>
        <script src="/assets/website/js/frontend/clashforios/custom.js"></script>
        <script src="https://www.freeclashnode.com/assets/js/frontend/invite-url.js"></script><script src="/assets/website/js/frontend/G.js"></script>
    </div>
    <!-- Main Wrapper End -->
</body>

</html>